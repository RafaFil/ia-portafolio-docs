<h1>KNN</h1>
<div class="container">
<p>
    Todo el conjunto de datos de entrenamiento se "memoriza" y, cuando es necesario clasificar registros de ejemplo sin etiquetar, los atributos de entrada de los nuevos registros sin etiquetar se comparan con todo el conjunto de entrenamiento para encontrar la coincidencia m√°s cercana. La etiqueta de clase del registro de entrenamiento m√°s cercano es la etiqueta de clase prevista para el registro de prueba no visto. Se trata de un m√©todo no param√©trico, en el que no se generaliza ni se intenta encontrar la distribuci√≥n del conjunto de datos (Altman, 1992). Una vez que los registros de entrenamiento est√°n en la memoria, la clasificaci√≥n del registro de prueba es muy sencilla. Hay que encontrar el registro de entrenamiento m√°s cercano para cada registro de prueba.
</p>

<p>
    Esta clase de aprendices adopta un enfoque contundente, en el que no se realiza ning√∫n "aprendizaje" a partir de datos de entrenamiento, sino que el conjunto de datos de entrenamiento se utiliza como tabla de consulta para ajustar las variables de entrada y encontrar el resultado. Estos enfoques se denominan aprendices perezosos.
</p>

<h2 class="mt-5">C√≥mo funciona:</h2>

<p>
    Cualquier registro de un conjunto de datos puede visualizarse como un punto en un espacio n-dimensional, donde n es el n√∫mero de atributos. ¬øQu√© ocurre cuando el punto de datos se encuentra en el l√≠mite de dos atributos de clases diferentes? La clasificaci√≥n puede ser complicada porque en la vecindad hay m√°s de una clase en las proximidades. Necesitamos un algoritmo eficaz para resolver estos casos y medir la proximidad de los puntos de datos con m√°s de dos dimensiones. Una t√©cnica consiste en encontrar el punto de datos de entrenamiento m√°s cercano a un punto de datos de prueba no visto en un espacio multidimensional, y utilizar el valor de la clase objetivo del punto de datos de entrenamiento m√°s cercano como la clase objetivo predicha para el punto de datos de prueba. Esto es similar al funcionamiento del algoritmo k-NN.
</p>

<p>
    La k del algoritmo k-NN indica el n√∫mero de registros de entrenamiento cercanos que deben tenerse en cuenta al realizar la predicci√≥n de un registro de prueba sin etiquetar. Cuando k = 1, el modelo intenta encontrar el primer registro m√°s cercano y adopta la etiqueta de clase del primer registro de entrenamiento m√°s cercano como valor de la clase objetivo predicha. Dado que la clase del registro objetivo se eval√∫a por votaci√≥n, a k se le suele asignar un n√∫mero impar para un problema de dos clases.
</p>

<h2 class="mt-5">Medida de proximidad:</h2>

<p>
    La eficacia del algoritmo k-NN depende de la determinaci√≥n de la semejanza o desemejanza entre un registro de prueba y el registro de entrenamiento memorizado. Una medida de proximidad entre dos registros es la medida de proximidad de sus atributos.
</p>

<h4 class="mt-5">Distancia</h4>
<ul>
            <li>Eucl√≠dea</li>
            <li>Manhattan</li>
            <li>Chebyshev</li>
            <li>Minkowski: (Generalizacion de las distancias)</li>
</ul>
<p>
$$
d = \left( \sum_{i=1}^n |x_i - y_i|^p \right)^{\frac{1}{p}}
$$
</p>
<p>
    Cuando p = 1, la medida de distancia es la distancia Manhattan, cuando p = 2 la medida de distancia es la distancia Eucl√≠dea, y cuando p = ‚àû la medida de distancia es la distancia Chebyshev.
</p>

<p>
    <strong>y‚Ä≤ = clase m√°xima(y1, y2, ‚ãØ ,yk)</strong>
</p>

<p><strong>Ponderaciones:</strong> Cuando k es m√°s de uno, se puede argumentar que los vecinos m√°s cercanos deber√≠an tener m√°s peso en el resultado de la clase objetivo predicha que los vecinos m√°s lejanos. Esto se puede conseguir asignando pesos a todos los vecinos, con los pesos aumentando a medida que los vecinos se acercan al punto de datos.</p>
<p>
w_i=\frac{e^{-d(x_in_i)}}{\sum_{i=1}^k e^{-d(x_in_i)}}
</p>
<p><strong>y = clase m√°xima(w1 * y1, w2 * y2, ‚ãØ , wk * yk)</strong></p>


<aside class="alert alert-info">
<strong>üí°Nota: Un problema de la aproximaci√≥n por distancia es que depende de la escala y las unidades de los atributos. Para mitigar el problema causado por las diferentes medidas y unidades, todas las entradas de k-NN suelen normalizarse, donde los valores de los datos se reescalan para ajustarse a un rango determinado. La normalizaci√≥n de todos los atributos proporciona una comparaci√≥n justa entre ellos.
</strong>
</aside>

<aside class="alert alert-info">
<strong>üí°Nota: La medida de distancia funciona bien para atributos num√©ricos. Sin embargo, si el atributo es categ√≥rico (nominal), la distancia entre dos puntos es 0 o 1. Si los valores de los atributos son los mismos, la distancia es 0. Si los valores de los atributos son iguales, la distancia es 0 y si los valores de los atributos son diferentes, la distancia es 1. Si el atributo es ordinal con m√°s de dos valores, los valores ordinales se pueden convertirse al tipo de datos entero con los valores 0, 1, 2, ..., n - 1 y el atributo convertido puede tratarse como un atributo num√©rico para el c√°lculo de la distancia.
</strong>
</aside>

<h4 class="mt-5">Similitud de Correlaci√≥n</h4>

<p>
    La correlaci√≥n entre dos puntos de datos X e Y es la medida de la relaci√≥n lineal entre los atributos X e Y.
</p>

<h4 class="mt-5">Coeficiente de Correspondencia Simple</h4>

<p>
    El coeficiente de correspondencia simple (CMS) se utiliza cuando los conjuntos de datos tienen atributos binarios. Por ejemplo, X es (1,1,0,0,1,1,0) e Y es (1,0,0,1,1,0,0). Podemos medir la similitud entre estos dos puntos de datos bas√°ndonos en la aparici√≥n simult√°nea de 0 o 1 con respecto al total de apariciones.
</p>

<h2 class="mt-5">Conclusi√≥n:</h2>

<p>
    El modelo k-NN requiere normalizaci√≥n para evitar cualquier sesgo por cualquier atributo que tenga unidades grandes o peque√±as en la escala. El modelo es bastante robusto cuando falta alg√∫n valor de atributo en el registro de prueba. Si falta el valor en el registro de prueba, se ignora todo el atributo en el modelo, y a√∫n as√≠ el modelo puede funcionar con una precisi√≥n razonable. En el ejemplo de aplicaci√≥n, si la
no se conoce la longitud del s√©palo de un registro de prueba, el modelo ignora la longitud del s√©palo. Cuando falta el valor de un atributo, k-NN se convierte en un modelo tridimensional en lugar de las cuatro dimensiones habituales.
Como aprendiz perezoso, la relaci√≥n entre la entrada y la salida no puede explicarse, ya que el modelo no es m√°s que un conjunto memorizado de todos los registros de entrenamiento. No hay generalizaci√≥n ni abstracci√≥n de la relaci√≥n.
</p>
</div>